"""
Embedding & Retrieval Testing Script
-------------------------------------
Untuk ngetes keseluruhan pipeline RAG

This script tests:
1. Configuration loading
2. Qdrant connection
3. Embedding generation (OpenAI/HuggingFace)
4. Vector indexing
5. Semantic search
6. Query engine
7. End-to-end RAG pipeline

Supports:
- OpenAI embeddings (paid)
- HuggingFace embeddings (FREE, local)
- Groq LLM (FREE)

Usage:
    python test_embedding_retrieval.py
"""

import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

from config.settings import settings
from vectorstore.qdrant_store import get_qdrant_client
from vectorstore.index_builder import IndexBuilder, EmbeddingGenerator
from rag.retriever import Retriever
from rag.query_engine import QueryEngine


def print_section(title: str):
    """Print section header."""
    print("\n" + "="*70)
    print(f"{'='*5} {title} {'='*5}")
    print("="*70 + "\n")


def test_configuration():
    """Test 1: Configuration loading."""
    print_section("TEST 1: CONFIGURATION")
    
    settings.display()
    
    # Check embedding provider
    print(f"\nüîç Embedding Provider: {settings.EMBEDDING_PROVIDER}")
    if settings.EMBEDDING_PROVIDER == "huggingface":
        print(f"   Model: {getattr(settings, 'HUGGINGFACE_MODEL', 'all-mpnet-base-v2')}")
        print(f"   Dimension: {settings.EMBEDDING_DIMENSION}")
        print("   ‚úÖ Using FREE local embeddings!")
    elif settings.EMBEDDING_PROVIDER == "openai":
        print(f"   Model: {settings.OPENAI_EMBEDDING_MODEL}")
        print(f"   Dimension: {settings.EMBEDDING_DIMENSION}")
        if settings.OPENAI_API_KEY:
            print("   ‚úÖ OpenAI API key found")
        else:
            print("   ‚ùå OpenAI API key missing!")
            return False
    
    # Check LLM provider
    print(f"\nü§ñ LLM Provider: {settings.LLM_PROVIDER}")
    if settings.LLM_PROVIDER == "groq":
        print(f"   Model: {settings.GROQ_LLM_MODEL}")
        print("   ‚úÖ Using FREE Groq LLM!")
    
    if settings.validate():
        print("\n‚úÖ Configuration is valid\n")
        return True
    else:
        print("\n‚ùå Configuration validation failed\n")
        print("üí° TIP: Make sure .env file is configured correctly")
        print("   See .env.example for reference\n")
        return False


def test_qdrant_connection():
    """Test 2: Qdrant connection."""
    print_section("TEST 2: QDRANT CONNECTION")
    
    client = None
    try:
        client = get_qdrant_client(create_collection=False)
        
        # Try to get collection info
        info = client.get_collection_info()
        
        print("‚úÖ Qdrant connection successful")
        print(f"   Collection: {info.get('name')}")
        print(f"   Points: {info.get('points_count')}")
        print(f"   Dimension: {info.get('vector_size')}")
        print("")
        
        result = True
        
    except Exception as e:
        print(f"‚ùå Qdrant connection failed: {e}\n")
        result = False
    
    finally:
        # IMPORTANT: Close the client to release the lock
        if client:
            client.close()
    
    return result, None  # Don't return client to avoid lock issues


def test_embedding_generation():
    """Test 3: Embedding generation."""
    print_section("TEST 3: EMBEDDING GENERATION")
    
    try:
        print(f"üîß Initializing {settings.EMBEDDING_PROVIDER} embedder...\n")
        generator = EmbeddingGenerator()
        
        # Test single embedding
        test_text = "This is a test sentence for embedding generation."
        print(f"üìù Test text: {test_text}\n")
        
        print("‚è≥ Generating embedding...")
        embedding = generator.generate_embedding(test_text)
        
        if embedding:
            print(f"\n‚úÖ Embedding generated successfully")
            print(f"   Provider: {settings.EMBEDDING_PROVIDER}")
            print(f"   Dimension: {len(embedding)}")
            print(f"   Expected: {settings.EMBEDDING_DIMENSION}")
            
            # Validate dimension
            if len(embedding) == settings.EMBEDDING_DIMENSION:
                print(f"   ‚úÖ Dimension matches!")
            else:
                print(f"   ‚ö†Ô∏è  Dimension mismatch! Update EMBEDDING_DIMENSION in .env")
            
            print(f"   First 5 values: {[f'{v:.4f}' for v in embedding[:5]]}")
            print("")
            return True, generator
        else:
            print("‚ùå Failed to generate embedding\n")
            return False, None
            
    except Exception as e:
        print(f"‚ùå Embedding generation failed: {e}\n")
        import traceback
        traceback.print_exc()
        return False, None


def test_indexing():
    """Test 4: Vector indexing."""
    print_section("TEST 4: VECTOR INDEXING")
    
    # Check if chunks exist
    chunks_file = settings.CHUNKS_DIR / "DataGlossary_chunks.json"
    
    if not chunks_file.exists():
        print(f"‚ö†Ô∏è  Chunks file not found: {chunks_file}")
        print("   Run chunking scripts first:")
        print("   > python src/ingestion/dataGlossary_chunking_preprocessing.py\n")
        return False
    
    try:
        print(f"üìÇ Chunks file: {chunks_file.name}\n")
        
        # Build index (limit for faster testing)
        # HuggingFace is slower than OpenAI, so use smaller batch
        if settings.EMBEDDING_PROVIDER == "huggingface":
            test_limit = 50  # Fewer chunks for local embeddings
            batch_size = 10
            print("   Using smaller batch for HuggingFace (slower but FREE)")
        else:
            test_limit = 100
            batch_size = 20
        
        builder = IndexBuilder()
        chunks = builder.load_chunks_from_json(str(chunks_file))
        
        # Limit for testing
        test_chunks = chunks[:test_limit] if len(chunks) > test_limit else chunks
        print(f"   Using {len(test_chunks)} chunks for testing")
        print(f"   Batch size: {batch_size}\n")
        
        success = builder.build_index(
            chunks=test_chunks,
            batch_size=batch_size,
            create_new_collection=True  # Recreate for clean test
        )
        
        if success:
            print("\n‚úÖ Indexing test passed\n")
            return True
        else:
            print("\n‚ùå Indexing test failed\n")
            return False
            
    except Exception as e:
        print(f"‚ùå Indexing failed: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_retrieval():
    """Test 5: Semantic search."""
    print_section("TEST 5: SEMANTIC SEARCH")
    
    try:
        # Initialize retriever
        print("üîß Initializing retriever...\n")
        retriever = Retriever()
        
        # Test query - relevant to the data glossary
        test_query = "Which tables use incremental extraction with watermark datetime?"
        print(f"üìù Test query: '{test_query}'\n")
        
        # Retrieve
        print("‚è≥ Searching vector store...")
        results = retriever.retrieve(test_query)
        
        if results:
            print(f"\n‚úÖ Retrieved {len(results)} results\n")
            
            # Show top result
            print("=" * 70)
            print("TOP RESULT:")
            print("=" * 70)
            top = results[0]
            print(f"Score: {top['score']:.4f}")
            print(f"Text: {top['text'][:200]}...")
            
            # Show metadata if available
            if top.get('metadata'):
                print(f"Metadata: {top['metadata']}")
            print("")
            
            # Show score distribution
            scores = [r['score'] for r in results]
            print("=" * 70)
            print("SCORE DISTRIBUTION:")
            print("=" * 70)
            print(f"Highest: {max(scores):.4f}")
            print(f"Lowest:  {min(scores):.4f}")
            print(f"Average: {sum(scores)/len(scores):.4f}")
            print("")
            
            # Show all results briefly
            print("=" * 70)
            print("ALL RESULTS:")
            print("=" * 70)
            for i, result in enumerate(results, 1):
                snippet = result['text'][:80].replace('\n', ' ')
                print(f"{i}. [{result['score']:.3f}] {snippet}...")
            
            print("\n‚úÖ Semantic search test passed\n")
            return True
        else:
            print("‚ùå No results returned\n")
            print("üí° TIP: Make sure indexing was successful and collection has data\n")
            return False
            
    except Exception as e:
        print(f"‚ùå Semantic search failed: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_query_engine():
    """Test 6: Query engine (RAG)."""
    print_section("TEST 6: QUERY ENGINE (RAG)")
    
    try:
        # Initialize query engine
        print("üîß Initializing query engine...\n")
        engine = QueryEngine()
        
        # Test question - relevant to the data glossary
        test_question = "What are the tables in the EMR database that use incremental extraction? List the table names and their watermark columns."
        print(f"‚ùì Test question: '{test_question}'\n")
        
        # Execute RAG query
        print("‚è≥ Running RAG pipeline (Retrieve + Generate)...\n")
        result = engine.query(test_question)
        
        if result['success']:
            print("\n‚úÖ RAG query successful!\n")
            
            # Show answer
            print("=" * 70)
            print("ANSWER:")
            print("=" * 70)
            print(result['answer'])
            print("")
            
            # Show sources
            print("=" * 70)
            print(f"SOURCES ({result['num_sources']} chunks):")
            print("=" * 70)
            for i, source in enumerate(result['sources'], 1):
                print(f"\n[{i}] Score: {source['score']:.3f}")
                snippet = source['text'][:150].replace('\n', ' ')
                print(f"    {snippet}...")
                
                # Show metadata if available
                metadata = source.get('metadata', {})
                if metadata and metadata.get('source'):
                    print(f"    Source: {metadata['source']}")
            
            print("\n" + "=" * 70)
            print("METADATA:")
            print("=" * 70)
            print(f"Model: {result['model']}")
            print(f"Provider: {result['provider']}")
            print(f"Avg Retrieval Score: {result['avg_score']:.3f}")
            print("=" * 70)
            
            print("\n‚úÖ Query engine test passed\n")
            return True
        else:
            print(f"‚ùå RAG query failed: {result.get('error', 'Unknown error')}\n")
            return False
            
    except Exception as e:
        print(f"‚ùå Query engine failed: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def run_all_tests():
    """Run all tests."""
    print("\n" + "üß™"*35)
    print("  RAG EMBEDDING & RETRIEVAL TEST SUITE")
    print("üß™"*35 + "\n")
    
    # Show configuration summary
    print("üìã Configuration Summary:")
    print(f"   Embeddings: {settings.EMBEDDING_PROVIDER}")
    if settings.EMBEDDING_PROVIDER == "huggingface":
        print(f"   Model: {getattr(settings, 'HUGGINGFACE_MODEL', 'all-mpnet-base-v2')}")
    print(f"   LLM: {settings.LLM_PROVIDER}")
    print(f"   Vector DB: Qdrant ({settings.QDRANT_MODE})")
    print("")
    
    results = {}
    
    # Test 1: Configuration
    results['configuration'] = test_configuration()
    if not results['configuration']:
        print("‚õî Stopping tests: Configuration invalid\n")
        return results
    
    # Test 2: Qdrant connection
    results['qdrant'], _ = test_qdrant_connection()
    if not results['qdrant']:
        print("‚ö†Ô∏è  Qdrant connection failed, but continuing with other tests\n")
    
    # Test 3: Embedding generation
    results['embedding'], generator = test_embedding_generation()
    if not results['embedding']:
        print("‚õî Stopping tests: Embedding generation failed\n")
        return results
    
    # Test 4: Indexing
    results['indexing'] = test_indexing()
    if not results['indexing']:
        print("‚ö†Ô∏è  Indexing failed, retrieval tests may not work\n")
    
    # Test 5: Retrieval
    if results['indexing']:
        results['retrieval'] = test_retrieval()
    else:
        results['retrieval'] = False
        print("‚è≠Ô∏è  Skipping retrieval test (no index)\n")
    
    # Test 6: Query engine
    if results['retrieval']:
        results['query_engine'] = test_query_engine()
    else:
        results['query_engine'] = False
        print("‚è≠Ô∏è  Skipping query engine test (retrieval failed)\n")
    
    # Summary
    print_section("TEST SUMMARY")
    
    print("Test Results:")
    print(f"   1. Configuration      : {'‚úÖ PASS' if results['configuration'] else '‚ùå FAIL'}")
    print(f"   2. Qdrant Connection  : {'‚úÖ PASS' if results['qdrant'] else '‚ùå FAIL'}")
    print(f"   3. Embedding Generation: {'‚úÖ PASS' if results['embedding'] else '‚ùå FAIL'}")
    print(f"   4. Vector Indexing    : {'‚úÖ PASS' if results['indexing'] else '‚ùå FAIL'}")
    print(f"   5. Semantic Search    : {'‚úÖ PASS' if results['retrieval'] else '‚ùå FAIL'}")
    print(f"   6. Query Engine       : {'‚úÖ PASS' if results['query_engine'] else '‚ùå FAIL'}")
    
    total = len(results)
    passed = sum(1 for v in results.values() if v)
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED! Your RAG pipeline is working correctly.")
        if settings.EMBEDDING_PROVIDER == "huggingface":
            print("   üí∞ 100% FREE setup using HuggingFace + Groq!")
        print("")
    elif passed >= total - 1:
        print("\n‚ö†Ô∏è  Most tests passed. Check failed tests above.\n")
    else:
        print("\n‚ùå Multiple tests failed. Review errors above.\n")
    
    return results


if __name__ == "__main__":
    results = run_all_tests()
    
    # Exit with appropriate code
    sys.exit(0 if all(results.values()) else 1)