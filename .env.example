# ============================================================
# RAG Chatbot MVP - Environment Configuration
# ============================================================
# Copy this file to .env and fill in your API keys
#
# IMPORTANT: Never commit .env to git (already in .gitignore)
# ============================================================

# ==================== API KEYS ====================
# OpenAI API Key (required if using OpenAI for LLM)
OPENAI_API_KEY=your_openai_api_key_here

# Groq API Key (required if using Groq for LLM)
GROQ_API_KEY=your_groq_api_key_here


# ==================== EMBEDDING CONFIGURATION ====================
# HuggingFace
EMBEDDING_PROVIDER=huggingface

# HuggingFace model options:
# - all-mpnet-base-v2 (768 dims, recommended, balanced)
# - all-MiniLM-L6-v2 (384 dims, fastest, lighter)
# - multi-qa-mpnet-base-dot-v1 (768 dims, best for Q&A)
HUGGINGFACE_MODEL=all-mpnet-base-v2

# Embedding dimension (must match model)
# all-mpnet-base-v2 = 768
# all-MiniLM-L6-v2 = 384
EMBEDDING_DIMENSION=768

# ==================== QDRANT CONFIGURATION ====================
# Qdrant mode: "local", "server", or "cloud"
# - local: Uses local SQLite storage (no web UI)
# - server: Connects to Qdrant server (Docker, with web UI)
# - cloud: Uses Qdrant Cloud (hosted, with web UI, butuh api key)
QDRANT_MODE=local

# Server/Cloud Qdrant URL
# For server mode (Docker): http://localhost:6333
# For cloud mode: https://your-cluster.cloud.qdrant.io:6333
QDRANT_URL=http://localhost:6333

# Cloud Qdrant API key (only needed if QDRANT_MODE=cloud)
QDRANT_API_KEY=your_qdrant_api_key_here

# Collection name
QDRANT_COLLECTION_NAME=rag_chatbot_chunks


# ==================== CHUNKING CONFIGURATION ====================
# Maximum characters per chunk
CHUNK_SIZE=512

# Overlap between chunks (in characters)
CHUNK_OVERLAP=50


# ==================== RETRIEVAL CONFIGURATION ====================
# Number of top results to retrieve
RETRIEVAL_TOP_K=5

# Minimum similarity score threshold (0.0 to 1.0)
MIN_SIMILARITY_SCORE=0.5


# ==================== LLM CONFIGURATION ====================
# LLM provider: "openai" or "groq" (tapi openai bayar cuy)
LLM_PROVIDER=groq

# OpenAI LLM models:
# - gpt-4o-mini (fast, cost-effective)
# - gpt-4o (most capable)
# - gpt-4-turbo
OPENAI_LLM_MODEL=gpt-4o-mini

# Groq LLM models:
# - llama-3.3-70b-versatile (recommended, fast)
# - llama-3.1-70b-versatile
# - mixtral-8x7b-32768
# - gemma2-9b-it
GROQ_LLM_MODEL=llama-3.3-70b-versatile

# LLM temperature (0.0 to 1.0)
# Lower = more focused, Higher = more creative
LLM_TEMPERATURE=0.7

# Maximum tokens in LLM response
LLM_MAX_TOKENS=1024

