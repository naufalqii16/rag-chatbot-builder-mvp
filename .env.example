# ==================== API KEYS ====================
# Get your free Groq API key at: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# OpenAI API key (optional, if using OpenAI models)
OPENAI_API_KEY=your_openai_api_key_here

# ==================== EMBEDDING CONFIGURATION ====================
# Provider: "huggingface" (FREE, recommended) or "openai"
EMBEDDING_PROVIDER=huggingface

# HuggingFace model (FREE, runs locally)
# Options:
# - sentence-transformers/all-mpnet-base-v2 (768 dims, best quality)
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims, fastest)
# - sentence-transformers/multi-qa-mpnet-base-dot-v1 (768 dims, optimized for Q&A)
HUGGINGFACE_MODEL=sentence-transformers/all-mpnet-base-v2

# OpenAI embedding model (if using OpenAI)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimension (must match your model)
# - all-mpnet-base-v2: 768
# - all-MiniLM-L6-v2: 384
# - text-embedding-3-small: 1536
EMBEDDING_DIMENSION=768

# ==================== LLM CONFIGURATION ====================
# Provider: "groq" (FREE, recommended) or "openai"
LLM_PROVIDER=groq

# Groq models (FREE):
# - llama-3.3-70b-versatile (recommended, best quality)
# - mixtral-8x7b-32768 (large context window)
# - gemma2-9b-it (fast, smaller model)
GROQ_LLM_MODEL=llama-3.3-70b-versatile

# LLM parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024

# ==================== QDRANT CONFIGURATION ====================
# Mode: "local" (recommended), "server", or "cloud"
QDRANT_MODE=local

# For server/cloud mode (optional)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your_qdrant_api_key_here

# Collection names
QDRANT_GLOSSARY_COLLECTION=glossary_collection
QDRANT_USER_UPLOAD_COLLECTION=user_upload_collection
QDRANT_COLLECTION_NAME=rag_chatbot_chunks

# ==================== CHUNKING CONFIGURATION ====================
# Chunk size in characters (smaller = more chunks, better precision)
CHUNK_SIZE=512

# Chunk overlap (maintains context between chunks)
CHUNK_OVERLAP=50

# ==================== FILE PROCESSING LIMITS ====================
# Maximum file size in MB (0 = no limit)
# Recommended: 50-100 MB for optimal performance
# For larger files: increase this value and PROCESSING_BATCH_SIZE
MAX_FILE_SIZE_MB=50

# Batch size for processing chunks (smaller = less memory usage)
# Recommended: 50-100 for normal files, 20-30 for very large files
PROCESSING_BATCH_SIZE=50

# Embedding batch size for API calls
# Recommended: 100 for HuggingFace, 50 for OpenAI (rate limits)
EMBEDDING_BATCH_SIZE=100

# ==================== RETRIEVAL CONFIGURATION ====================
# Number of chunks to retrieve for each query
RETRIEVAL_TOP_K=5

# Minimum similarity score (0.0 to 1.0)
# Lower = more results but less relevant
# Higher = fewer results but more relevant
MIN_SIMILARITY_SCORE=0.5

# ==================== TIPS FOR LARGE FILES ====================
# For files > 50 MB:
# 1. Increase MAX_FILE_SIZE_MB to your file size
# 2. Decrease PROCESSING_BATCH_SIZE to 20-30 (less memory)
# 3. Use HuggingFace embeddings (FREE, no API limits)
# 4. Consider splitting very large files (>500 MB)
#
# For files > 100 MB:
# - CHUNK_SIZE=512, PROCESSING_BATCH_SIZE=20, EMBEDDING_BATCH_SIZE=50
#
# For files > 500 MB:
# - Consider splitting into multiple files
# - Or use CHUNK_SIZE=1024 (fewer chunks)
